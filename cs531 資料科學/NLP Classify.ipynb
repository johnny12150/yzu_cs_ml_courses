{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Wade\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.768 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'高鐵辦 捐血 活動   董座 江耀宗 帶頭 挽袖'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = '高鐵辦捐血活動 董座江耀宗帶頭挽袖'\n",
    "seg = jieba.cut(words)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加特定詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"user.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移除符號(像是冒號與問號)和數字\n",
    "python regex移除數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "some_text = 'test:123?'\n",
    "re.sub(\"\\d+|\\?|\\:\", \"\", some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀取csv並切割字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "\n",
    "strings = []\n",
    "labels = []\n",
    "\n",
    "with open('sentences.csv', newline='') as csvfile:\n",
    "  rows = csv.reader(csvfile)\n",
    "  for row in rows:\n",
    "    strings.append(\" \".join(jieba.cut(re.sub(\"\\d+|\\?|\\:|\\？|\\\\r\\\\n\", \"\", row[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['高鐵辦 捐血 活動   董座 江耀宗 帶頭 挽袖',\n",
       " '高鐵局 ： 延伸 花 億   到 屏東 只 快 分鐘',\n",
       " '高鐵 延至 屏東 恐花 億元 、 卻 只 比台 鐵快 分鐘',\n",
       " '機 捷通 車   台北 車站 成大迷 宮台 鐵 ： 將改進',\n",
       " '高鐵 捐血 傳愛     董事 長 帶頭作 公益',\n",
       " '高雄捷 運去 年度 首度 盈餘萬元',\n",
       " '桃捷 徵才 逾 萬人 報考     錄取 率僅 %',\n",
       " '爭取 高鐵到 屏東   青年 組聯盟 號召 支持',\n",
       " '高捷 賺 了     衝運量 兼 擴大勞務',\n",
       " '機捷 A 台北 站 轉乘     吉祥物 VR 影片 來 帶路',\n",
       " '催淚   視障 女 高鐵站 為 阿母 獻 唱 「 感謝 你 的 愛 」',\n",
       " '周一 晨間 列車 自由 座     高鐵 增為 節車 廂',\n",
       " '高鐵班 周一 晨間 列車 加開 自由 座   運能 提升',\n",
       " '北捷 雞 年 紀念 車票     日開 賣',\n",
       " '傳聞 多   桃 市府 日 公布 機捷 正式 通車 時間',\n",
       " '機捷 何時 通車     桃捷 日 宣布',\n",
       " '機捷 可望 元宵 通車   桃捷 ： 尚未 取得 營運許 可 \\u3000',\n",
       " '機捷 履勘 改善 完成     交部將 發營 運許 可',\n",
       " '三鐵共站 動線 不 友善     議員促 左營 站 改善',\n",
       " '二二八 連假 高鐵 加班 車     日 凌晨 賣',\n",
       " '高鐵 周年   外籍 顧問 自發 揪 團回 台灣',\n",
       " '高鐵連假 加班 車   周六 開放 訂票',\n",
       " '機場 捷運營 運許 可及 通車 時間     日 公布',\n",
       " '張 花冠 宣布   日起 至 元宵 節兩條 電動 公車 路線 免費 搭',\n",
       " '機場 捷運何 時通 鄭文燦 ： 今年 月初',\n",
       " '機捷 試營運 方案   下午 A 站 說 明細節',\n",
       " '記者 團試 乘 機捷 直達車   實測 到 機場 時間 是 ...',\n",
       " '機捷試 乘遇 地震     減速 慢行 秒',\n",
       " '桃園 機場 捷運 ／ 試營運   ／ 正式 通車',\n",
       " '機捷 / 起試 營運個 月   / 正式 營運',\n",
       " '機捷 試營運 / 預辦 登機   限時 後 航班',\n",
       " '機捷 免費試 乘團 體限站   自由 行全線 進出',\n",
       " '機 捷通 車成 真     賀陳旦 開懷 大笑',\n",
       " '機捷 直達車 跟 普通 車   看顏色 就 知道',\n",
       " '機捷何 時開 放免 費試 乘     下午 公布',\n",
       " '高鐵春節 疏運 啟動     天班',\n",
       " '女警 「 聞雞 起舞 」   高鐵 左營 站 示範 防身 舞',\n",
       " '小年夜 除夕     高鐵加 開列 全車 自由 座',\n",
       " '台 中捷 運綠線 首列 電聯車 ／ 抵 台中港     ／ 遊行 開放民眾 參觀',\n",
       " '高鐵今 加開 兩班 北上 全 自由 座列車',\n",
       " '高鐵疏 運北返 旅客     午後加 開班車',\n",
       " '機捷 日 起試 營運     先開 放團 體試 乘',\n",
       " '機捷 團體 試營運 啟動     車頭 風光 搶 先 看',\n",
       " '機捷 試營運     有人 新鮮 有人 不 舒服',\n",
       " '台 中文 心路 路平 孔蓋 完成 下 地     首次 成功 納入 寬頻 網路',\n",
       " '機捷 啟動給 幾分   鄭文燦 ：',\n",
       " '高鐵 親友 團試 乘 機捷 ： 上 下坡 平穩',\n",
       " '機捷 試營運 Uber 暫停   鄭運鵬喻 交通 奇特 日',\n",
       " '元宵 賞燈   高鐵加開 班列 車',\n",
       " '台灣 燈會 高鐵加 開班車   成 自由 座',\n",
       " '機捷 今起 試營 運團體 首發   鄭文燦 ： 感受 平穩 舒適',\n",
       " '華泰 名品 城天 年 假   營收 逾 億',\n",
       " '機捷 很搖   鄭文燦 ： 多慮 了   不會 坐雲霄 飛車',\n",
       " '機捷試 乘     前 半月 團體 後 半月 個 人',\n",
       " '手機 沒電     機捷 直達車 可無線 充電',\n",
       " '搜集 試營 運數據   機捷 ： 請民眾 多用 卡',\n",
       " '機捷 試營運     首班 車 乘客 樂歡 天',\n",
       " '機場 捷運團 體試 營運     免費試 乘',\n",
       " '時分 發車   桃園 捷運 首班 車人 搶 搭',\n",
       " '桃捷 企業識別 隱含 T 意象     運輸 科技 信賴',\n",
       " '月 日 中視 午間 新聞 搶 先 看',\n",
       " '桃捷 拒 絕民眾 自行 組團試 乘     雙方 僵持 分',\n",
       " '搭 高鐵賞 台灣 燈會   日 開放 購票',\n",
       " '月 日 各 報頭 版要 聞',\n",
       " '打造 城際 接駁     喜迎 台灣 燈會',\n",
       " '搭機捷 女性 看 這裡   A 站化妝室 超豪華',\n",
       " '機捷 隔音 牆 反釀 巨大 噪音   高鐵局 ： 個 月 內 完成 評估',\n",
       " '台北 燈節 登場   西門捷 運站 運量 萬人',\n",
       " '機捷略 搖晃     桃捷 ： 科學 數據 逐步 調整',\n",
       " '月 日 中視 午間 新聞 搶 先 看',\n",
       " '高鐵贈 票助 病童   寒假 親子 遊台 中',\n",
       " '台灣 燈會將 登場     台鐵高 鐵 都 加班',\n",
       " '熟年 優惠 擴及 到 歲 高鐵 澄清 ： 謠傳 ！',\n",
       " '中捷 綠線 首列 電聯車     運抵 台中港',\n",
       " '桃捷 ： 歡迎民團 、 沿線 里民 組團 報名試 乘',\n",
       " '桃捷 ： 通車 後 不 宣導手 扶梯 靠右 站',\n",
       " '台 中捷 運首輛 電聯車 抵 台中港   日遊 街睹 風采',\n",
       " '小小 鐵道 迷 搭 機捷     關懷 協會 嘗鮮',\n",
       " '白化 症鐵道 迷試 乘 機捷   桃捷 研擬 放大 字體',\n",
       " '中捷 G 有 站 無路 林佳龍 ： 繞 一下 還是 找 的 到 路',\n",
       " '機 捷通 車在 即   長 庚 醫院 站 去年 房價漲 . ％ 稱霸',\n",
       " '機捷 免費試 乘   台茂日 推 假日 接駁車',\n",
       " '無法 在家 等 收貨   智能 櫃進 駐 超商 、 高捷',\n",
       " '機捷 試營運     鼓勵 免費刷 電子票',\n",
       " '迎開學   高鐵 推大學生 返校 折 優惠',\n",
       " '陳 文淵接 勤益 校長   林陵 三盼 為 中捷 舉才',\n",
       " '搶攻 機場 捷運 商機   百貨業 推新 企劃',\n",
       " '高鐵邀 賞蘭展     車票 折 優惠 套票',\n",
       " '重慶國 小舞 獅隊   高鐵台 中 站 快 閃 賀歲',\n",
       " '高鐵 鼓勵 旅客 「 由 『 港 』 而發 」   免費 車票 天天 抽',\n",
       " '高鐵 南港 站 打 知名度     送 免費 乘車 兌換券',\n",
       " '桃捷 無障礙 設施   身障 團體 ： 還有 改善 空間',\n",
       " '因應 燈會 、 國際馬拉松   高捷 延長 、 提前 發車',\n",
       " '台北 燈節 遊行展 開   北捷 西門 站 增萬運量',\n",
       " '高鐵車 廂 字幕   竟 出現 「 誰家 老婆 上 錯床 」',\n",
       " '鬧 元宵 看煙火   高雄 交管 運輸 到 凌晨 時',\n",
       " '台 中捷 運 首列 車廂   遊行 亮相',\n",
       " '搶 看 燈會   高鐵雲林 站 進出 站 人數 破紀錄',\n",
       " '曾 抱怨 不必 多次 動工   柯 P 今 再 為 萬大線 舉行 動土',\n",
       " '燈會 熱潮     高鐵雲林 站 單日 破 . 萬人',\n",
       " '台 中捷 運綠線 首列 車   今 展示 遊行 亮相',\n",
       " '中捷 電聯車 市區 遊行     上千 民眾爭 相 拍照',\n",
       " '環狀線 施工   板 新路 到 日 晚間 時 雙向 封閉 小時',\n",
       " '機捷 試營運   每天 萬 人次 參加',\n",
       " '顏值 最高 桃捷試 乘團   亮相',\n",
       " '推動捷 運黃線     高 市府 結合 立委 出擊',\n",
       " '機場 捷運日 起     開放 自由 試乘',\n",
       " '預掛 行李 好 輕 鬆   桃機 市區 預辦 登機 服務 啟用',\n",
       " '機捷 日 全線 自由 試乘   每日 限量 萬人',\n",
       " '搭機捷 玩 台茂   免費 接駁 再 送元',\n",
       " '機捷 開放個 人試 乘   柯 P 體 驗 讚 品 質 好',\n",
       " '台茂 機捷 A 假日 接駁車   好禮加',\n",
       " '機捷 提供 無線 充電   柯文 哲 ： 有 改善 空間',\n",
       " '高捷 要 鋪纜     提升 隧道 通訊頻 寬',\n",
       " '預辦 登機系 統明 啟用   桃機將 評估 擴大 服務 可行性',\n",
       " '柯 P 視察 行旅 廣場   要 西區 門戶 計畫 趕 在世 大運前 完成',\n",
       " '騎士 擦 撞 輕 軌致 掉 漆     高捷 不 排除 求償',\n",
       " '機捷 預辦 登機   海關 進駐 台北 站',\n",
       " '機捷 試營運   新北 新闢 條捷 運公車 路線',\n",
       " '慶祝桃 捷個 人 自由 試乘   桃機 各廠 商祭 優惠',\n",
       " '機場 捷運日 開放 自由 試乘   每天 限量 約 萬人',\n",
       " '高捷 跨足 經營 電信 事業   NCC 准 了',\n",
       " '機捷 開放個 人試 乘   台茂日 免費 接駁 再 送 商品 券',\n",
       " '機捷 自由 試乘   台北 站 每時 段 張',\n",
       " '機捷 開放民眾 自由 試乘     發放萬號 碼牌',\n",
       " '機捷個 人試 乘 登場     台北 站 大 排長 龍體驗',\n",
       " '機捷 自由 試乘     民眾 開心 排隊 嘗鮮',\n",
       " '機捷個 人試 乘     時段 抽號 碼牌',\n",
       " '市區 預辦 登機     不 搭 機捷 也 可免 費用',\n",
       " '機捷 預辦 登機 快速       託 運 行李 件 分鐘',\n",
       " '高鐵 二二八 假期 增班 尖峰 加班 車   日 開訂',\n",
       " '機捷 今 開放 自由 試乘   民眾 大 讚 平 穩',\n",
       " '高鐵連 假加 開班車   周六 開放 購票',\n",
       " '機捷 自由 行     站 人潮 搶 搶 滾',\n",
       " '機捷試 乘   預辦 登機 託 運 行李 一次 搞懂',\n",
       " '機 捷通 車   北市 消保官 嚴查 台北 地下街 商家',\n",
       " '機捷 A 票發 完   阿基師 來 送 餐卷',\n",
       " '桃園 高鐵站 國際 客試 搭   讚 機 捷 便利  ',\n",
       " '機捷 體驗     這些 站 與 時段 人 較 少',\n",
       " '機捷 自由 試乘 首日   返北車 末班 車人 擠 人',\n",
       " '機捷 自由 試乘 首日   一航廈 站 閘門 被 撞 壞',\n",
       " '外國 使節 團試 乘 機捷   鄭文燦 ： 積極 與 世界 交流',\n",
       " '機捷 今 開放 自由 試乘   民眾 早起 搶頭 香',\n",
       " '機捷 開放試 乘 第二天   民眾 踴躍體驗',\n",
       " '假期 高鐵 疏運   加開班車',\n",
       " '台灣 高鐵經驗 海外 輸出   首次 競標 失利',\n",
       " '台茂 搶機 捷通 車 人潮   免費 接駁 車今 首航',\n",
       " '桃園 大溪 往返 捷運永寧 站   日 啟用',\n",
       " '機 捷通 車   世紀 不動產 ： 桃園市 房價 最快 年 有 機會 追上 新北',\n",
       " '機捷 AA 緊急 避難車 站 鋼軌 斷裂   桃捷 ： 不影響 安全',\n",
       " '桃捷 檢修 發現 鋼 軌裂 縫     暫採魚 尾板 夾 固定',\n",
       " '機捷 鋼軌 發現 裂縫   高鐵局 ： 安全 無虞',\n",
       " '美化 新店 安坑 輕 軌捷 運圍 籬   學子 各展 創意',\n",
       " '機捷裂 軌換 好   委原廠 及 第三 單位 調查',\n",
       " '捷運 環狀線 施工   中 和 、 板橋 交維',\n",
       " '機捷 裂軌 完成 更換     將送德 原廠 檢定 原因',\n",
       " '搶機捷 美食 商機   京站 吃 壽司 送 折券',\n",
       " '捷運綠線 G 站     延伸 到 中壢 火車 站',\n",
       " '桃園 平鎮 到 新北 更 快     快捷 上路',\n",
       " '桃市 快捷 公車線 平鎮 往返 捷運永寧 站   明日 通車',\n",
       " '高鐵 苗栗 站 送 車票   一圓教 養院 生出 遊夢想',\n",
       " '台北 捷運局 而立   推動 大學 開設 捷運 課程',\n",
       " '連假 交通 服務   高鐵雲林 站 免費 接駁 沿海',\n",
       " '二二八 連假   高鐵加開 自由 座列車',\n",
       " '迎連假   高鐵 今明 增開 兩 加班 車',\n",
       " '周五 綠運輸 日 贈點   柯 P ： 用 更 大 力量 吸引',\n",
       " '淡海 輕軌 被 柯文 哲酸   新北捷 運局 反駁',\n",
       " '清理 空調 設備   北捷 研發 「 伸縮 自如 手臂 」',\n",
       " '機捷 大道 設計案     月底 招標',\n",
       " '二二八 連假     出國試 乘機 捷省 時間',\n",
       " '機捷 A 站 漏水   桃捷 要求 高鐵局 進行 全面 檢查',\n",
       " '機捷長 庚站 漏水     桃捷 ： 委請 高鐵局 處置',\n",
       " '北捷 列車 傳 巨響 冒 煙   疏散 名 乘客',\n",
       " '機捷 小 確幸     航廈 機場 旅館 往返 零 扣款',\n",
       " '機捷團 購百張 打折   通勤 族 必看',\n",
       " '桃捷 「 通勤 」 團體 優惠   比季票 更 便宜',\n",
       " '機捷車 站 漏水     排水 堵塞 溢流 所致',\n",
       " '因應 連假試 乘 人潮   機捷 明起 延長 至 晚間 時',\n",
       " '桃捷 日 至 /   延長 試乘 到 晚上 時',\n",
       " '機捷 疏 於 保養 導致 漏水   高鐵局 ： 依約 請 廠 商盡快 修 復',\n",
       " '機捷 車廂 進出 溫差 達度     旅客 不適',\n",
       " '機捷 新北轉 乘公車線   Ubike 前分 免費',\n",
       " '視察機捷     鄭文燦 ： 試營 運展現 交通 效益',\n",
       " '進出 站 太急   桃捷 高鐵桃園 站票 口閘門 被 撞 壞',\n",
       " '連假 收假     高鐵 下午 晚間 加開 列車',\n",
       " '機捷 最 後 一天 免費 搭     籲民眾 把握',\n",
       " '機捷 轉乘 公車線   Youbike 前分 免費',\n",
       " '民眾 熱情 捧場     賀陳旦 ： 機捷 最佳 分數',\n",
       " '機捷 試營 運打 幾分 賀陳旦 ： 民眾捧場 是 最好 評分',\n",
       " '機 捷通 車     交長 ： 運量 絕非 靠 運價 打折',\n",
       " '機 捷通 車     首 月票 價折',\n",
       " '機 捷通 車     推桃 囍 通車 紀念 套票',\n",
       " '機捷試 乘 一個 月     乘客 萬 人次',\n",
       " '機捷 日 正式 通車 營運   A 行李 處理區 首 曝光',\n",
       " '機捷 正式 營運   北車 轉乘 分鐘 搞定',\n",
       " '機捷 營運     首月 折 另 有 優惠 專案',\n",
       " '鄭文燦 ： 機捷試 乘   共萬 人次',\n",
       " '機捷 開通   首班 旅客 ： 出國 更 方便 了',\n",
       " '機場 捷運 正式 通車   首 月票 價折',\n",
       " '機捷 正式 通車 首日   鄭文燦要 大家 乖乖 搭車']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(2000)\n",
    "tokenizer.fit_on_texts(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[220, 124, 221, 222, 223, 224, 225],\n",
       " [27, 2, 125, 226, 126, 15, 80, 127, 81, 51],\n",
       " [8, 227, 80, 228, 229, 28, 230, 127, 231, 232, 51],\n",
       " [16, 17, 9, 18, 233, 234, 235, 128, 2, 236],\n",
       " [8, 124, 237, 238, 129, 239, 240],\n",
       " [241, 242, 243, 244, 245],\n",
       " [6, 246, 130, 39, 247, 248, 249],\n",
       " [250, 251, 80, 252, 253, 254, 255],\n",
       " [29, 256, 52, 257, 258, 259],\n",
       " [1, 20, 18, 3, 82, 260, 261, 262, 131, 263],\n",
       " [264, 265, 266, 132, 83, 267, 268, 269, 30, 270, 271, 133, 272, 31],\n",
       " [134, 135, 53, 5, 54, 8, 273, 274, 136],\n",
       " [275, 134, 135, 53, 84, 5, 54, 276, 137],\n",
       " [55, 277, 85, 138, 56, 278, 139],\n",
       " [279, 280, 281, 140, 7, 86, 1, 21, 11, 57],\n",
       " [1, 282, 11, 6, 7, 141],\n",
       " [1, 283, 58, 11, 6, 2, 284, 285, 286, 142, 287],\n",
       " [1, 288, 59, 40, 289, 290, 143, 142],\n",
       " [291, 292, 41, 293, 294, 144, 3, 59],\n",
       " [60, 42, 8, 43, 9, 7, 145, 139],\n",
       " [8, 295, 296, 297, 298, 299, 300, 32],\n",
       " [301, 43, 9, 146, 22, 302],\n",
       " [12, 303, 143, 304, 11, 57, 7, 86],\n",
       " [147, 305, 141, 306, 87, 58, 307, 308, 309, 148, 13, 23],\n",
       " [12, 310, 311, 24, 2, 312, 313],\n",
       " [1, 14, 314, 88, 20, 3, 315, 316],\n",
       " [317, 89, 4, 1, 90, 318, 15, 12, 57, 149],\n",
       " [44, 319, 320, 321, 322, 323],\n",
       " [45, 12, 33, 61, 14, 61, 21, 11],\n",
       " [1, 150, 324, 34, 21, 35],\n",
       " [1, 14, 25, 36, 325, 62, 326],\n",
       " [1, 91, 151, 327, 5, 328, 63],\n",
       " [16, 17, 329, 330, 92, 331, 332],\n",
       " [1, 90, 333, 334, 9, 335, 336, 337],\n",
       " [338, 339, 340, 341, 4, 88, 86],\n",
       " [342, 152, 153, 343],\n",
       " [344, 30, 345, 346, 31, 8, 144, 3, 347, 348, 349],\n",
       " [350, 351, 154, 352, 353, 5, 54],\n",
       " [46, 19, 155, 64, 65, 61, 156, 93, 61, 66, 157, 354],\n",
       " [355, 84, 356, 357, 358, 5, 158],\n",
       " [359, 360, 67, 361, 94],\n",
       " [1, 7, 150, 35, 362, 363, 159, 4],\n",
       " [1, 68, 14, 153, 364, 365, 26, 95, 47],\n",
       " [1, 14, 160, 366, 160, 41, 367],\n",
       " [46, 368, 369, 370, 371, 40, 372, 373, 161, 374, 375, 376, 377],\n",
       " [1, 378, 162, 24, 2],\n",
       " [8, 379, 89, 4, 1, 2, 163, 380, 164],\n",
       " [1, 14, 381, 382, 383, 96, 384, 7],\n",
       " [58, 385, 165, 386, 9],\n",
       " [32, 37, 154, 94, 387, 5, 54],\n",
       " [1, 388, 69, 389, 390, 24, 2, 391, 164, 392],\n",
       " [393, 394, 395, 85, 396, 397, 130, 126],\n",
       " [1, 398, 24, 2, 399, 52, 400, 401, 402],\n",
       " [44, 4, 403, 166, 68, 62, 166, 167, 70],\n",
       " [404, 405, 1, 90, 406, 168],\n",
       " [407, 69, 408, 1, 2, 409, 410, 411],\n",
       " [1, 14, 97, 9, 98, 412, 413],\n",
       " [12, 414, 159, 35, 91, 4],\n",
       " [415, 169, 45, 33, 97, 170, 26, 23],\n",
       " [6, 416, 417, 418, 419, 171, 420, 421],\n",
       " [34, 7, 172, 173, 174, 26, 95, 47],\n",
       " [6, 422, 423, 424, 425, 4, 426, 427, 428],\n",
       " [23, 429, 32, 37, 7, 22, 175],\n",
       " [34, 7, 430, 431, 432, 433],\n",
       " [434, 435, 48, 436, 32, 37],\n",
       " [176, 437, 47, 438, 20, 439, 440],\n",
       " [1, 441, 442, 443, 444, 445, 27, 2, 167, 34, 446, 40, 177],\n",
       " [18, 178, 99, 447, 448, 179, 39],\n",
       " [449, 450, 6, 2, 451, 452, 453, 454],\n",
       " [34, 7, 172, 173, 174, 26, 95, 47],\n",
       " [455, 456, 457, 458, 459, 460, 100],\n",
       " [32, 461, 99, 462, 128, 463, 43],\n",
       " [464, 38, 465, 15, 466, 8, 467, 2, 468, 469],\n",
       " [19, 470, 64, 65, 471, 93],\n",
       " [6, 2, 472, 28, 473, 474, 475, 476, 4],\n",
       " [6, 2, 11, 62, 41, 477, 478, 479, 3],\n",
       " [46, 19, 480, 65, 156, 93, 481, 482, 483],\n",
       " [484, 485, 486, 23, 1, 487, 488, 180],\n",
       " [489, 490, 491, 4, 1, 6, 492, 493, 494],\n",
       " [19, 181, 71, 3, 495, 496, 2, 497, 498, 499, 500, 133, 15, 501],\n",
       " [16, 17, 502, 503, 129, 504, 505, 3, 506, 507, 508, 509],\n",
       " [1, 91, 4, 182, 510, 183, 184],\n",
       " [511, 512, 513, 514, 515, 516, 517, 518, 28, 29],\n",
       " [1, 14, 185, 519, 520],\n",
       " [521, 8, 522, 523, 101, 38],\n",
       " [524, 525, 526, 527, 528, 529, 83, 19, 530],\n",
       " [531, 12, 33, 186, 532, 533, 534],\n",
       " [535, 536, 56, 101, 38, 187],\n",
       " [537, 538, 539, 540, 100, 3, 81, 541, 542],\n",
       " [8, 185, 67, 30, 543, 544, 545, 546, 547, 31, 13, 56, 548, 549],\n",
       " [8, 550, 3, 551, 552, 49, 13, 553, 554],\n",
       " [6, 555, 556, 557, 68, 2, 558, 59, 188],\n",
       " [189, 37, 28, 559, 29, 102, 28, 560, 169],\n",
       " [18, 178, 561, 562, 55, 563, 3, 564],\n",
       " [565, 136, 566, 567, 568, 30, 569, 570, 163, 571, 31],\n",
       " [572, 58, 573, 574, 575, 171, 15, 145, 72],\n",
       " [46, 19, 103, 64, 190, 66, 104],\n",
       " [26, 47, 37, 105, 3, 63, 3, 576, 577],\n",
       " [578, 579, 580, 581, 582, 73, 74, 75, 106, 83, 583, 584, 585],\n",
       " [37, 586, 105, 3, 587, 588, 39],\n",
       " [46, 19, 155, 64, 9, 75, 589, 66, 104],\n",
       " [19, 65, 107, 66, 590, 591, 592, 593],\n",
       " [191, 192, 594, 595, 15, 7, 108, 72, 596, 597, 598],\n",
       " [1, 14, 193, 194, 109, 599],\n",
       " [600, 601, 602, 151, 104],\n",
       " [603, 604, 605, 140, 606, 607, 608],\n",
       " [12, 195, 609, 22, 5, 10],\n",
       " [610, 76, 110, 111, 611, 196, 107, 25, 36, 112, 113],\n",
       " [1, 7, 612, 5, 10, 613, 197, 39],\n",
       " [176, 614, 114, 13, 48, 106, 615],\n",
       " [1, 198, 77, 4, 73, 74, 616, 617, 115, 618, 619, 110],\n",
       " [114, 1, 20, 183, 184, 620],\n",
       " [1, 621, 622, 168, 199, 623, 2, 71, 59, 188],\n",
       " [29, 200, 624, 137, 625, 626, 627],\n",
       " [25, 628, 629, 113, 630, 177, 631, 112, 632],\n",
       " [73, 74, 633, 634, 635, 200, 636, 637, 638, 639, 640, 641, 40],\n",
       " [642, 643, 116, 111, 644, 645, 646, 29, 41, 647, 648],\n",
       " [1, 25, 36, 649, 650, 18, 3],\n",
       " [1, 14, 117, 651, 652, 653, 148],\n",
       " [654, 655, 70, 5, 10, 196, 656, 657, 38],\n",
       " [12, 195, 22, 5, 10, 193, 197, 658, 39],\n",
       " [29, 659, 660, 661, 662, 663, 664, 52],\n",
       " [1, 198, 77, 4, 182, 13, 48, 106, 49, 665, 666],\n",
       " [1, 5, 10, 18, 3, 667, 668, 147],\n",
       " [1, 157, 5, 10, 669, 201],\n",
       " [202, 77, 4, 99, 18, 3, 118, 670, 671],\n",
       " [1, 5, 10, 50, 672, 673, 180],\n",
       " [202, 77, 4, 203, 674, 201],\n",
       " [107, 25, 36, 41, 23, 1, 675, 676, 677],\n",
       " [1, 25, 36, 678, 204, 103, 76, 679, 51],\n",
       " [8, 60, 205, 680, 681, 43, 9, 7, 682],\n",
       " [1, 75, 22, 5, 10, 50, 118, 115, 683, 684],\n",
       " [685, 686, 94, 146, 22, 175],\n",
       " [1, 5, 687, 3, 119, 26, 26, 688],\n",
       " [44, 4, 25, 36, 204, 103, 76, 689, 690],\n",
       " [16, 17, 9, 691, 692, 693, 18, 694, 695],\n",
       " [1, 20, 696, 697, 698, 131, 49, 699],\n",
       " [45, 132, 700, 701, 23, 115, 16, 702, 703],\n",
       " [1, 704, 705, 3, 206, 203, 70, 706, 707],\n",
       " [1, 5, 10, 120, 708, 709, 170, 710, 70],\n",
       " [1, 5, 10, 120, 711, 3, 712, 121, 116, 207],\n",
       " [713, 714, 89, 4, 1, 24, 2, 715, 206, 716, 717],\n",
       " [1, 75, 22, 5, 10, 50, 718, 719, 720],\n",
       " [1, 721, 4, 722, 50, 723],\n",
       " [205, 8, 152, 724],\n",
       " [32, 725, 726, 727, 161, 728, 729],\n",
       " [114, 730, 17, 9, 119, 13, 48, 731, 732],\n",
       " [45, 733, 122, 208, 3, 7, 113],\n",
       " [16, 17, 9, 734, 735, 2, 736, 737, 738, 85, 71, 739, 740, 117],\n",
       " [1, 741, 742, 743, 3, 209, 744, 6, 2, 745, 210],\n",
       " [6, 746, 211, 747, 748, 749, 750, 751, 752, 753],\n",
       " [1, 209, 211, 754, 27, 2, 210, 755],\n",
       " [756, 757, 758, 111, 759, 760, 761, 762, 763, 764],\n",
       " [765, 766, 110, 767, 768, 769, 770, 771],\n",
       " [33, 191, 192, 100, 772, 28, 773, 774],\n",
       " [1, 775, 40, 776, 777, 778, 779, 780],\n",
       " [781, 782, 186, 783, 784, 785, 49, 786],\n",
       " [787, 181, 3, 125, 15, 788, 789, 3],\n",
       " [45, 212, 15, 117, 78, 81, 213, 790],\n",
       " [791, 213, 214, 212, 122, 208, 3, 792, 11],\n",
       " [8, 793, 3, 49, 56, 794, 795, 796, 797],\n",
       " [18, 798, 799, 800, 801, 802, 33, 803],\n",
       " [42, 96, 112, 105, 3, 13, 48, 804],\n",
       " [60, 42, 165, 5, 158],\n",
       " [805, 8, 806, 807, 808, 43, 9],\n",
       " [809, 810, 7, 811, 73, 74, 2, 812, 78, 118, 813, 814],\n",
       " [815, 816, 121, 199, 817, 818, 819, 820],\n",
       " [821, 822, 823, 55, 824, 30, 825, 826, 827, 31],\n",
       " [1, 828, 829, 830, 831],\n",
       " [60, 42, 832, 833, 834, 57],\n",
       " [1, 20, 3, 79, 6, 835, 27, 836, 837, 838],\n",
       " [839, 840, 79, 6, 2, 841, 27, 842],\n",
       " [55, 53, 843, 844, 845, 846, 847, 848, 98],\n",
       " [1, 849, 850, 851, 12, 852, 122, 853, 854],\n",
       " [855, 856, 215, 216, 857, 858],\n",
       " [6, 30, 216, 31, 68, 38, 859, 78, 860],\n",
       " [861, 3, 79, 862, 863, 864, 865],\n",
       " [189, 866, 4, 119, 1, 867, 102, 87, 108, 72],\n",
       " [6, 7, 87, 102, 10, 15, 868, 72],\n",
       " [1, 869, 870, 871, 872, 79, 27, 2, 873, 874, 875, 876, 877, 878],\n",
       " [1, 190, 63, 879, 880, 67, 881],\n",
       " [1, 882, 883, 884, 217, 13],\n",
       " [885, 24, 2, 69, 886, 96, 887],\n",
       " [63, 3, 888, 6, 889, 890, 891, 121, 116, 207],\n",
       " [42, 892, 8, 88, 108, 84, 53],\n",
       " [1, 893, 62, 894, 13, 23, 895, 896],\n",
       " [1, 82, 214, 897, 217, 13],\n",
       " [50, 898, 899, 92, 2, 1, 900, 901],\n",
       " [1, 69, 902, 162, 92, 2, 903, 149, 904, 905],\n",
       " [16, 17, 9, 906, 2, 179, 907, 908, 909, 215],\n",
       " [16, 17, 9, 123, 218, 219],\n",
       " [16, 17, 9, 910, 911, 11, 138, 187],\n",
       " [44, 4, 912, 34, 98, 194, 109],\n",
       " [1, 7, 21, 11, 35, 20, 76, 913, 123, 914],\n",
       " [1, 21, 35, 915, 82, 51, 916],\n",
       " [1, 35, 917, 101, 918, 71, 38, 919],\n",
       " [24, 2, 44, 4, 920, 109],\n",
       " [1, 921, 97, 67, 2, 922, 78, 923, 52],\n",
       " [12, 33, 21, 11, 123, 218, 219],\n",
       " [1, 21, 11, 120, 924, 925, 926, 927]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_sequences = tokenizer.texts_to_sequences(strings)\n",
    "string_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 讀取切好字的csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Title_CKIP</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>捷運間隔5分鐘放人進站　人潮塞爆國父紀念館站</td>\n",
       "      <td>捷運　間隔　5分鐘　放　人　進站　人潮　塞爆　國父　紀念館　站</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>高捷延至兩點收班　夢時代跨年人潮1小時散去</td>\n",
       "      <td>高捷　延至　兩　點收　班　夢時代　跨年　人潮　1　小時　散去</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31日北捷運量226.7萬人　比前年少40萬人</td>\n",
       "      <td>31　日　北　捷運　量　226.7　萬　人　比　前　年少　40萬　人</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>北捷跨年輸運減少38萬人次　是因假日沒上班族</td>\n",
       "      <td>北捷　跨年　輸運　減少　38萬　人次　是　因　假日　沒　上班族</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>元旦連假收尾　高鐵烏日站午後湧人潮</td>\n",
       "      <td>元旦　連假　收尾　高鐵　烏日站　午後　湧　人潮</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title                          Title_CKIP Category\n",
       "0   捷運間隔5分鐘放人進站　人潮塞爆國父紀念館站     捷運　間隔　5分鐘　放　人　進站　人潮　塞爆　國父　紀念館　站        t\n",
       "1    高捷延至兩點收班　夢時代跨年人潮1小時散去      高捷　延至　兩　點收　班　夢時代　跨年　人潮　1　小時　散去        t\n",
       "2  31日北捷運量226.7萬人　比前年少40萬人  31　日　北　捷運　量　226.7　萬　人　比　前　年少　40萬　人        t\n",
       "3   北捷跨年輸運減少38萬人次　是因假日沒上班族     北捷　跨年　輸運　減少　38萬　人次　是　因　假日　沒　上班族        t\n",
       "4        元旦連假收尾　高鐵烏日站午後湧人潮             元旦　連假　收尾　高鐵　烏日站　午後　湧　人潮        t"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DF = pd.read_csv('title1-8_final.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title         1\n",
       "Title_CKIP    1\n",
       "Category      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop整row na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segement = DF['Title_CKIP'].copy()\n",
    "segement = segement.astype(str)\n",
    "label = DF['Category'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label做one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "['x0_e' 'x0_o' 'x0_s' 'x0_t']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# le = LabelEncoder()\n",
    "# label_le = le.fit_transform(label)\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "label_ohe = one_hot.fit_transform(DF[['Category']])\n",
    "label_ohe_array = label_ohe.toarray()\n",
    "print(label_ohe_array)\n",
    "print(one_hot.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'捷運\\u3000間隔\\u30005分鐘\\u3000放\\u3000人\\u3000進站\\u3000人潮\\u3000塞爆\\u3000國父\\u3000紀念館\\u3000站'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segement.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**將\\u3000以空格取代**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for index in range(segement.size):\n",
    "    segement.values[index] = re.sub(\"\\\\u3000\", \" \", segement.values[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'捷運 間隔 5分鐘 放 人 進站 人潮 塞爆 國父 紀念館 站'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segement.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(5000)\n",
    "tokenizer.fit_on_texts(segement.values)\n",
    "string_sequences = tokenizer.texts_to_sequences(segement.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**截長補短**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1752, 1753,    8],\n",
       "       [   0,    0,    0, ...,   71,  175, 1755],\n",
       "       [   0,  761,   44, ..., 1757, 1758,   18],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1748, 1749,    8],\n",
       "       [   0,    0,    0, ..., 1748, 1749,    8],\n",
       "       [   0,    0,    0, ...,  542,  743,  970]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "sequence_padded = sequence.pad_sequences(string_sequences, maxlen=15)\n",
    "sequence_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1452, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**檢查詞頻**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv_result = cv.fit_transform(segement.values)\n",
    "\n",
    "words = cv.get_feature_names()\n",
    "# 計算詞頻\n",
    "frequency = cv_result.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>高鐵</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>捷運</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>機捷</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>北捷</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>加開</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>列車</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>疏運</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>優惠</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>通車</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>旅客</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Words  Frequency\n",
       "3455    高鐵        653\n",
       "1530    捷運        222\n",
       "1941    機捷        154\n",
       "708     北捷        129\n",
       "677     加開        117\n",
       "624     列車        112\n",
       "2283    疏運         74\n",
       "521     優惠         68\n",
       "3056    通車         63\n",
       "1734    旅客         61"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordDF =  pd.DataFrame(\n",
    "    {'Words': words,\n",
    "     'Frequency': frequency\n",
    "    })\n",
    "\n",
    "WordDF.sort_values(by=['Frequency'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "將數字list轉化為vector list\n",
    "\n",
    "#### input length由句子長度決定, input dim由字典數決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 15, 32)            160000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               61568     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 222,084\n",
      "Trainable params: 222,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_MLP = Sequential()\n",
    "\n",
    "model_MLP.add(Embedding(output_dim=32, input_dim=5000, input_length=15))\n",
    "model_MLP.add(Dropout(0.2))\n",
    "\n",
    "model_MLP.add(Flatten())\n",
    "\n",
    "model_MLP.add(Dense(units=128, activation='relu' ))\n",
    "model_MLP.add(Dropout(0.4))\n",
    "\n",
    "model_MLP.add(Dense(units=4, activation='softmax' ))\n",
    "\n",
    "model_MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1160 samples, validate on 291 samples\n",
      "Epoch 1/40\n",
      " - 0s - loss: 0.0026 - acc: 0.9983 - val_loss: 2.6041 - val_acc: 0.5979\n",
      "Epoch 2/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9974 - val_loss: 2.6197 - val_acc: 0.5945\n",
      "Epoch 3/40\n",
      " - 0s - loss: 0.0023 - acc: 0.9983 - val_loss: 2.6701 - val_acc: 0.5911\n",
      "Epoch 4/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9974 - val_loss: 2.7507 - val_acc: 0.5945\n",
      "Epoch 5/40\n",
      " - 0s - loss: 0.0033 - acc: 0.9983 - val_loss: 2.7814 - val_acc: 0.5945\n",
      "Epoch 6/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9991 - val_loss: 2.7724 - val_acc: 0.5979\n",
      "Epoch 7/40\n",
      " - 0s - loss: 0.0032 - acc: 0.9974 - val_loss: 2.7256 - val_acc: 0.5979\n",
      "Epoch 8/40\n",
      " - 0s - loss: 0.0021 - acc: 0.9983 - val_loss: 2.7563 - val_acc: 0.5911\n",
      "Epoch 9/40\n",
      " - 0s - loss: 0.0029 - acc: 0.9991 - val_loss: 2.8068 - val_acc: 0.5911\n",
      "Epoch 10/40\n",
      " - 0s - loss: 0.0037 - acc: 0.9966 - val_loss: 2.7423 - val_acc: 0.6014\n",
      "Epoch 11/40\n",
      " - 0s - loss: 0.0025 - acc: 0.9974 - val_loss: 2.7207 - val_acc: 0.5979\n",
      "Epoch 12/40\n",
      " - 0s - loss: 0.0027 - acc: 0.9983 - val_loss: 2.6965 - val_acc: 0.5945\n",
      "Epoch 13/40\n",
      " - 0s - loss: 0.0032 - acc: 0.9974 - val_loss: 2.6875 - val_acc: 0.5979\n",
      "Epoch 14/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9974 - val_loss: 2.7204 - val_acc: 0.5945\n",
      "Epoch 15/40\n",
      " - 0s - loss: 0.0020 - acc: 0.9991 - val_loss: 2.7696 - val_acc: 0.5911\n",
      "Epoch 16/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9983 - val_loss: 2.7829 - val_acc: 0.5945\n",
      "Epoch 17/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9974 - val_loss: 2.8246 - val_acc: 0.5979\n",
      "Epoch 18/40\n",
      " - 0s - loss: 0.0022 - acc: 0.9974 - val_loss: 2.8269 - val_acc: 0.5979\n",
      "Epoch 19/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9983 - val_loss: 2.8043 - val_acc: 0.5945\n",
      "Epoch 20/40\n",
      " - 0s - loss: 0.0025 - acc: 0.9974 - val_loss: 2.8057 - val_acc: 0.5945\n",
      "Epoch 21/40\n",
      " - 0s - loss: 0.0025 - acc: 0.9983 - val_loss: 2.7675 - val_acc: 0.5979\n",
      "Epoch 22/40\n",
      " - 0s - loss: 0.0024 - acc: 0.9983 - val_loss: 2.7794 - val_acc: 0.5945\n",
      "Epoch 23/40\n",
      " - 0s - loss: 0.0026 - acc: 0.9974 - val_loss: 2.8223 - val_acc: 0.5979\n",
      "Epoch 24/40\n",
      " - 0s - loss: 0.0037 - acc: 0.9974 - val_loss: 2.7752 - val_acc: 0.5979\n",
      "Epoch 25/40\n",
      " - 0s - loss: 0.0023 - acc: 0.9983 - val_loss: 2.7624 - val_acc: 0.5945\n",
      "Epoch 26/40\n",
      " - 0s - loss: 0.0024 - acc: 0.9974 - val_loss: 2.8438 - val_acc: 0.5979\n",
      "Epoch 27/40\n",
      " - 0s - loss: 0.0042 - acc: 0.9966 - val_loss: 2.8058 - val_acc: 0.5979\n",
      "Epoch 28/40\n",
      " - 0s - loss: 0.0024 - acc: 0.9983 - val_loss: 2.8158 - val_acc: 0.5979\n",
      "Epoch 29/40\n",
      " - 0s - loss: 0.0025 - acc: 0.9974 - val_loss: 2.8482 - val_acc: 0.6014\n",
      "Epoch 30/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9983 - val_loss: 2.8070 - val_acc: 0.6014\n",
      "Epoch 31/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9966 - val_loss: 2.8775 - val_acc: 0.6014\n",
      "Epoch 32/40\n",
      " - 0s - loss: 0.0023 - acc: 0.9974 - val_loss: 2.9126 - val_acc: 0.5979\n",
      "Epoch 33/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9974 - val_loss: 2.8471 - val_acc: 0.5979\n",
      "Epoch 34/40\n",
      " - 0s - loss: 0.0022 - acc: 0.9991 - val_loss: 2.8962 - val_acc: 0.6048\n",
      "Epoch 35/40\n",
      " - 0s - loss: 0.0024 - acc: 0.9983 - val_loss: 2.9564 - val_acc: 0.6082\n",
      "Epoch 36/40\n",
      " - 0s - loss: 0.0030 - acc: 0.9966 - val_loss: 2.9317 - val_acc: 0.6048\n",
      "Epoch 37/40\n",
      " - 0s - loss: 0.0027 - acc: 0.9983 - val_loss: 2.9381 - val_acc: 0.6048\n",
      "Epoch 38/40\n",
      " - 0s - loss: 0.0033 - acc: 0.9974 - val_loss: 2.9363 - val_acc: 0.6048\n",
      "Epoch 39/40\n",
      " - 0s - loss: 0.0028 - acc: 0.9991 - val_loss: 2.9520 - val_acc: 0.6014\n",
      "Epoch 40/40\n",
      " - 0s - loss: 0.0046 - acc: 0.9974 - val_loss: 2.8652 - val_acc: 0.5979\n"
     ]
    }
   ],
   "source": [
    "model_MLP.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "train_history_MLP = model_MLP.fit(sequence_padded, label_ohe,batch_size=50, epochs=40,verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 15, 128)           640000    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 15, 32)            20736     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 15, 32)            8448      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, 32)                8448      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                1650      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 679,486\n",
      "Trainable params: 679,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional, CuDNNLSTM\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "\n",
    "model_LSTM.add(Embedding(output_dim=128, input_dim=5000, input_length=15))\n",
    "model_LSTM.add(Dropout(0.4))\n",
    "\n",
    "model_LSTM.add(CuDNNLSTM(32, return_sequences=True))\n",
    "model_LSTM.add(CuDNNLSTM(32, return_sequences=True))\n",
    "model_LSTM.add(CuDNNLSTM(32, return_sequences=False))\n",
    "\n",
    "model_LSTM.add(Dense(units=50, activation='relu' ))\n",
    "model_LSTM.add(Dropout(0.4))\n",
    "model_LSTM.add(Dense(units=4, activation='softmax' ))\n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**有做one hot的label就不能用loss = sparse_categorical_crossentropy**\n",
    "\n",
    "> https://stackoverflow.com/questions/49392972/error-when-checking-target-expected-dense-3-to-have-shape-3-but-got-array-wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1160 samples, validate on 291 samples\n",
      "Epoch 1/30\n",
      " - 1s - loss: 0.0121 - acc: 0.9974 - val_loss: 2.8684 - val_acc: 0.5945\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0095 - acc: 0.9974 - val_loss: 2.8206 - val_acc: 0.5979\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0095 - acc: 0.9974 - val_loss: 3.0308 - val_acc: 0.5945\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0081 - acc: 0.9974 - val_loss: 3.4984 - val_acc: 0.5979\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0068 - acc: 0.9974 - val_loss: 3.0688 - val_acc: 0.5773\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0058 - acc: 0.9974 - val_loss: 3.0980 - val_acc: 0.5773\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0045 - acc: 0.9974 - val_loss: 3.2512 - val_acc: 0.5911\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0043 - acc: 0.9983 - val_loss: 3.3558 - val_acc: 0.5876\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0057 - acc: 0.9983 - val_loss: 3.4013 - val_acc: 0.6082\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0052 - acc: 0.9974 - val_loss: 3.5947 - val_acc: 0.6014\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0062 - acc: 0.9983 - val_loss: 3.4107 - val_acc: 0.5876\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0039 - acc: 0.9991 - val_loss: 3.5049 - val_acc: 0.5808\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0044 - acc: 0.9983 - val_loss: 3.6079 - val_acc: 0.5773\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0033 - acc: 0.9991 - val_loss: 3.6644 - val_acc: 0.5601\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0052 - acc: 0.9974 - val_loss: 3.7678 - val_acc: 0.5704\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0031 - acc: 0.9974 - val_loss: 3.9029 - val_acc: 0.5636\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0030 - acc: 0.9983 - val_loss: 4.0058 - val_acc: 0.5670\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0035 - acc: 0.9991 - val_loss: 4.1304 - val_acc: 0.5464\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0042 - acc: 0.9983 - val_loss: 3.9458 - val_acc: 0.5808\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0049 - acc: 0.9957 - val_loss: 4.0974 - val_acc: 0.5704\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0095 - acc: 0.9966 - val_loss: 3.6855 - val_acc: 0.6014\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0159 - acc: 0.9948 - val_loss: 2.6785 - val_acc: 0.5704\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0062 - acc: 0.9974 - val_loss: 2.7931 - val_acc: 0.5739\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0076 - acc: 0.9957 - val_loss: 3.1021 - val_acc: 0.5704\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0062 - acc: 0.9966 - val_loss: 3.2700 - val_acc: 0.5670\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0229 - acc: 0.9948 - val_loss: 3.2894 - val_acc: 0.5601\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0135 - acc: 0.9940 - val_loss: 3.2670 - val_acc: 0.5739\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0120 - acc: 0.9957 - val_loss: 3.1139 - val_acc: 0.5739\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0065 - acc: 0.9983 - val_loss: 3.1864 - val_acc: 0.5773\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0039 - acc: 0.9983 - val_loss: 3.2707 - val_acc: 0.5808\n"
     ]
    }
   ],
   "source": [
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "train_history_LSTM = model_LSTM.fit(sequence_padded, label_ohe, batch_size=100, epochs=30, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
